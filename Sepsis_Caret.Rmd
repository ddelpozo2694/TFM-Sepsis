---
title: "TFM Detección Sepsis"
author: "David del Pozo Granados"
output:
  pdf_document:
    toc: yes
  html_document:
    df_print: paged
    toc: yes
    toc_float:
      collapsed: no
    number_sections: yes
    theme: cosmo
---
## Lectura de datos

En primer lugar cargaremos la libería caret y borraremos los que tengamos actualmente en el espacio de trabajo. Posteriormente, leeremos los datos de entrenamiento, test y validación que hemos convertido de Pkl a Csv para poder tratarlos con el paquete caret

```{r}
rm(list = ls())
```


```{r}
library(caret)
```


```{r}
library(dplyr) # for data manipulation
library(caret) # for model-building
library(purrr) # for functional programming (map)
library(pROC) # for AUC calculations
library(PRROC) # for Precision-Recall curve calculations
```


```{r}

test_data <- read.csv("csv_data/linear_interpolation/test_data_ML.csv")
train_data <- read.csv("csv_data/linear_interpolation/train_data_ML.csv")
val_data <- read.csv("csv_data/linear_interpolation/val_data_ML.csv")

test_data_eti <- read.csv("csv_data/linear_interpolation/test_data_etiqueta.csv")
train_data_eti <- read.csv("csv_data/linear_interpolation/train_data_etiqueta.csv")
val_data_eti <- read.csv("csv_data/linear_interpolation/val_data_etiqueta.csv")

```

En este procesamiento elegimos el subconjunto de datos que queremos usar en función del número de horas. Si elegimos por ejemplo 3 horas, tendremos 4 tomas de datos para cada una de las 44 medidas (3 horas + onset)

```{r}

n_horas <- 24 #Elegimos el número de horas antes de onset que elegimos.
j<-0
valores <- c()
i<-0

while(i<44){
  valores_aux <- c(j:(j+n_horas))
  valores <- c(valores,valores_aux)
  valores_aux <- c()
  j <- j +49
  i <- i+1
}
valores <- valores +1
```


Quitaremos la primera columna de cada conjunto de datos, puesto que es simplemente un identificador del paciente que no nos ofrece información relevantes para el entrenamiento de los modelos. Posteriormente también quitaremos las medidas para las horas que no queremos, es decir, desde la hora posterior elegida en n_horas hasta 49.

```{r}

test_data <- test_data[,-1]
test_data <- test_data[,valores]

train_data <- train_data[,-1]
train_data <- train_data[,valores]

val_data <- val_data[,-1]
val_data <- val_data[,valores]



test_data_eti <- test_data_eti[,-1]
train_data_eti <- train_data_eti[,-1]
val_data_eti <- val_data_eti[,-1]
```

Añadimos las etiquetas al conjunto de datos de entrenamiento y  factorizamos las etiquetas que indican con un 1 si el paciente finalmente se le detectó sepsis o con un 0 en caso contrario.

```{r}
train_data<- cbind(train_data,train_data_eti)
test_data<- cbind(test_data,test_data_eti)
val_data<- cbind(val_data,val_data_eti)

test_data$test_data_eti<- factor(test_data$test_data_eti, levels=c(1,0),labels=c("SI","NO"))
train_data$train_data_eti<-factor(train_data$train_data_eti, levels=c(1,0),labels=c("SI","NO"))
val_data$val_data_eti<- factor(val_data$val_data_eti, levels=c(1,0),labels=c("SI","NO"))

```

Sacamos cuantos pacientes tienen o no Sepsis en nuestros tres conjuntos de datos:

**Conjunto de datos de entrenamiento**

```{r}
# Valor entero
table(train_data$train_data_eti)

# Valor porcentual
prop.table(table(train_data$train_data_eti))

```

**Conjunto de datos de test**

```{r}
# Valor entero
table(test_data$test_data_eti)

# Valor porcentual
prop.table(table(test_data$test_data_eti))

```


**Conjunto de datos de validación**

```{r}
# Valor entero
table(val_data$val_data_eti)

# Valor porcentual
prop.table(table(val_data$val_data_eti))

```

Creamos una función de control con los siguientes parámetros:
- **Método**: validación cruzada 
- **Tamaño fold**: 10
- **Guardar predicciones**: TRUE
- **Modo Verbose**: FALSE
- Finalmente, el parámetro summaryFunction y classProbs deben indicar que queremos calcular curvas ROC y para ello nuestro modelo debe proporcionar no solo predicción de clase sino probabilidad de la predicción:
  - **summaryFunction** = twoClassSummary,
  - **classProbs** = TRUE

```{r}
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
```


Entrenamos los distintos modelos basados en algoritmos de clasificación que hemos seleccionado y que nos ofrece el paquete caret:

- **KNN**
- **GLMNET_TUNING**
- **GLMBOOST**
- **RPART**
- **CTREE2**
- **NAIVE BAYES**

```{r}
#ENTRENAMOS MODELO KNN

mod_knn <- train(train_data_eti~.,
                 data = train_data,
                 method = "knn",
                  trControl = ctrl)

save(mod_knn, file="mod_knn.RData")
```



```{r}
#ENTRENAMOS MODELO LOGISTIC REGRESSION
 mod_LogitBoost <- train(
   train_data_eti~.,
  data = train_data,
   method = "LogitBoost",
                  trControl = ctrl)
save(mod_LogitBoost, file="mod_LogitBoost.RData")
```



```{r}
#ENTRENAMOS MODELO RANDOM FOREST

 mod_rf <- train(
   train_data_eti~.,
  data = train_data,
   method = "rf",
                  trControl = ctrl)
save(mod_rf, file="mod_rf.RData")
```


```{r}
#ENTRENAMOS MODELO GLMBOOST
 
mod_glmboost <- train(   train_data_eti~.,
                            data = train_data,
                             method = "glmboost",
                  trControl = ctrl)

save(mod_glmboost, file="mod_glmboost.RData")

```

```{r}
#ENTRENAMOS MODELO RPART
mod_rpart <- train(   train_data_eti~.,
                            data = train_data,
                             method = "rpart", 
                  trControl = ctrl)

save(mod_rpart, file="mod_rpart.RData")
```



```{r}
#ENTRENAMOS MODELO CTREE2
mod_ctree2<- train(   train_data_eti~.,
                            data = train_data,
                             method = "ctree2",
                  trControl = ctrl)

save(mod_ctree2, file="mod_ctree2.RData")
```

```{r}
 #ENTRENAMOS MODELO NAIVE BAYES
mod_nb <- train(   train_data_eti~.,
                            data = train_data,
                             method = "naive_bayes",
                  trControl = ctrl)

save(mod_nb, file="mod_nb.RData")
```



```{r}
#ENTRENAMOS MODELO XGBTree 
mod_xgbTree <- train(   train_data_eti~.,
                            data = train_data,
                            method = "xgbTree",
                         trControl = ctrl,
                         verbose = FALSE,
                         verbosity = 0)

save(mod_xgbTree, file="mod_xgbTree.RData")
```


```{r}
#ENTRENAMOS MODELO svmLinear2
mod_svmLinear2 <- train(   train_data_eti~.,
                          data = train_data,
                            method = "svmLinear2", 
                            trControl = ctrl)

save(mod_svmLinear2, file="mod_svmLinear2.RData")
```

```{r}
save(mod_rpart, file="mod_rpart.RData")
save(mod_ctree2, file="mod_ctree2.RData")
save(mod_nb, file="mod_nb.RData")
save(mod_xgbTree, file="mod_xgbTree.RData")
save(mod_svmLinear2, file="mod_svmLinear2.RData")
```


```{r}
load("mod_rpart.RData")
load("mod_ctree2.RData")
load("mod_nb.RData")
load("mod_xgbTree.RData")
load("mod_svmLinear2.RData")
load("mod_knn.RData")
load("mod_glmboost.RData")
load("mod_LogitBoost.RData")
```


Se irá añadiendo a una lista los distintos modelos que hemos ido entrenando y estos se pasarán por un remuestreo con la función resamples.

```{r}
Modelos_Entrenados <- list(
   knn = mod_knn,
   glmboost = mod_glmboost,
   rpart = mod_rpart,
   ctree2 = mod_ctree2,
   naive_bayes = mod_nb,
   xgbTree = mod_xgbTree,
   svmLinear2 = mod_svmLinear2,
   LogitBoost = mod_LogitBoost
   #bstTree = mod_bstTree
   )


resamples <- resamples(Modelos_Entrenados)

```

Haremos una predicción de los modelos anteriormente entrenados, usando el conjunto de datos de validación para calculara la precisión de los modelos (Accuracy):

```{r}

#Accuracy para modelo knn
knn_pred <- predict(mod_knn, test_data)
postResample(pred = knn_pred, obs = test_data$test_data_eti)

#Accuracy para modelo glmnet_tuning
#glmnet_tuning_pred <- predict(mod_glmnet_tuning, test_data)
#postResample(pred = glmnet_tuning_pred, obs = test_data$test_data_eti)

#Accuracy para modelo glmboost
glmboost_pred <- predict(mod_glmboost, test_data)
postResample(pred = glmboost_pred, obs = test_data$test_data_eti)

#Accuracy para modelo rpart
rpart_pred <- predict(mod_rpart, test_data)
postResample(pred = rpart_pred, obs = test_data$test_data_eti)

#Accuracy para modelo ctree2
ctree2_pred <- predict(mod_ctree2, test_data)
postResample(pred = ctree2_pred, obs = test_data$test_data_eti)

#Accuracy para modelo nb
nb_pred <- predict(mod_nb, test_data)
postResample(pred = nb_pred, obs = test_data$test_data_eti)

#Accuracy para modelo xgbTree
xgbTree_pred <- predict(mod_xgbTree, test_data)
postResample(pred = xgbTree_pred , obs = test_data$test_data_eti)

#Accuracy para modelo svmLinear
svmLinear_pred <- predict(mod_svmLinear2, test_data)
postResample(pred = svmLinear_pred , obs = test_data$test_data_eti)

#Accuracy para modelo LogitBoost
LogitBoost_pred <- predict(mod_LogitBoost, test_data)
postResample(pred = LogitBoost_pred , obs = test_data$test_data_eti)
```



```{r}

#Confusion Matriz para modelo knn
knn_pred <- predict(mod_knn, test_data)
confusionMatrix(knn_pred ,test_data$test_data_eti,mode="everything")

#Confusion Matriz para modelo glmnet_tuning
#glmnet_tuning_pred <- predict(mod_glmnet_tuning, test_data)
#confusionMatrix(glmnet_tuning_pred ,test_data$test_data_eti)

#Confusion Matriz para modelo glmboost
glmboost_pred <- predict(mod_glmboost, test_data)
confusionMatrix(glmboost_pred ,test_data$test_data_eti,mode="everything")

#Confusion Matriz para modelo rpart
rpart_pred <- predict(mod_rpart, test_data)
confusionMatrix(rpart_pred ,test_data$test_data_eti,mode="everything")

#Confusion Matriz para modelo ctree2
ctree2_pred <- predict(mod_ctree2, test_data)
confusionMatrix(ctree2_pred ,test_data$test_data_eti,mode="everything")

#Confusion Matriz para modelo naive bayes
nb_pred <- predict(mod_nb, test_data)
confusionMatrix(nb_pred  ,test_data$test_data_eti,mode="everything")

#Confusion Matriz para modelo xgbTree
xgbTree_pred <- predict(mod_xgbTree, test_data)
confusionMatrix(xgbTree_pred  ,test_data$test_data_eti,mode="everything")

#Confusion Matriz para modelo svmLinear2
svmLinear2_pred <- predict(mod_svmLinear2, test_data)
confusionMatrix(svmLinear2_pred  ,test_data$test_data_eti,mode="everything")

#Confusion Matriz para modelo LogitBoost
LogitBoost_pred <- predict(mod_LogitBoost, test_data)
confusionMatrix(LogitBoost_pred  ,test_data$test_data_eti,mode="everything")
```




Mostramos gráficamente los modelos y los compararemos para ver de manera visual cual es el mejor modelo de predicción temprana de sepsis para este conjunto de datos. Con el siguiente código mostraremos el área bajo la curva ROC, la sensibilidiad y la especificidad.

```{r}

summary(resamples)
bwplot(resamples)
dotplot(resamples)
```

Calculamos AUPRC

```{r}
calc_auprc <- function(model, data){
  
  index_class2 <- data$train_data_eti == "SI"
  index_class1 <- data$train_data_eti == "NO"
  
  predictions <- predict(model, data, type = "prob")
  
  pr.curve(predictions$SI[index_class2], predictions$SI[index_class1], curve = TRUE)
  
}
```

```{r}
model_list_pr <- Modelos_Entrenados %>%
  map(calc_auprc, data = train_data)
model_list_pr %>%
  map(function(the_mod) the_mod$auc.integral)
```
Mostramos gráficamos los resultamos de AUPRC para los distintos modelos

```{r}

results_list_pr <- list(NA)
num_mod <- 1
for(the_pr in model_list_pr){
  
  results_list_pr[[num_mod]] <- data_frame(recall = the_pr$curve[, 1],
                                           precision = the_pr$curve[, 2],
                                           model = names(model_list_pr)[num_mod])
  
  num_mod <- num_mod + 1
  
}
results_df_pr <- bind_rows(results_list_pr)
custom_col <- c("#000000", "#009E73", "#0072B2", "#D55E00","#CC79A7","#CCBB79","#AB0A28","#7FCC79")
ggplot(aes(x = recall, y = precision, group = model), data = results_df_pr) +
  geom_line(aes(color = model), size = 1) +
  scale_color_manual(values = custom_col) +
  geom_abline(intercept = sum(test_data$Class == "Class2")/nrow(test_data),
              slope = 0, color = "gray", size = 1) +
  theme_bw()
```


